# Model Setting
model: WideDeep
embedding_size: 32                     # (int) The embedding size of features.
mlp_hidden_size: [32, 16, 8]           # (list of int) The hidden size of MLP layers.
dropout_prob: 0.2                      # (float) The dropout rate.

# Atomic File Format
field_separator: "\t"                  # (str) Separator of different columns in atomic files.
seq_separator: " "                     # (str) Separator inside the sequence features.

# Basic Information
USER_ID_FIELD: user_id                 # (str) Field name of user ID feature.
ITEM_ID_FIELD: item_id                 # (str) Field name of item ID feature.
RATING_FIELD: ~                        # (str) Field name of rating feature.
TIME_FIELD: timestamp                  # (str) Field name of timestamp feature.
seq_len: ~                             # (dict) Field name of sequence feature: maximum length of each sequence
LABEL_FIELD: label                     # (str) Expected field name of the generated labels for point-wise dataLoaders. 
threshold: ~                           # (dict) 0/1 labels will be generated according to the pairs.
NEG_PREFIX: neg_                       # (str) Negative sampling prefix for pair-wise dataLoaders.

# Selectively Loading
load_col:                              # (dict) The suffix of atomic files: (list) field names to be loaded.
  inter: [user_id, item_id, timestamp, label]
  item: [item_id, title, genres, directors, writers, year]

# Negative Sampling for Implicit Feedback
train_neg_sample_args:                 # (dict) Arguments for negative sampling during training.
  distribution: 'uniform'              # (str) Sampling distribution: 'uniform' or 'popularity'
  sample_num: 4                        # (int) Number of negative samples per positive sample.
valid_neg_sample_args:                 # (dict) Arguments for negative sampling during validation.
  distribution: 'uniform'              # (str) Sampling distribution: 'uniform' or 'popularity'
  sample_num: 1                        # (int) Number of negative samples per positive sample in validation.

# Evaluation Settings
eval_args:                             # (dict) 4 keys: group_by, order, split, and mode
  split: {'RS':[0.99,0.01,0]}          # (dict) The splitting strategy ranging in ['RS','LS'].
  group_by: user                       # (str) The grouping strategy ranging in ['user', 'none'].
  order: RO                            # (str) The ordering strategy ranging in ['RO', 'TO'].
  mode: full                           # (str) The evaluation mode ranging in ['full','unixxx','popxxx','labeled'].
repeatable: False                      # (bool) Whether to evaluate results with a repeatable recommendation scene. 
metrics: ["Recall","NDCG"]
topk: [10]                             # (list or int or None) The value of k for topk evaluation metrics.
valid_metric: Recall@10                # (str) The evaluation metric for early stopping. 
valid_metric_bigger: True              # (bool) Whether to take a bigger valid metric value as a better result.
eval_batch_size: 131072                # (int) The evaluation batch size.
metric_decimal_place: 4                # (int) The decimal place of metric scores.
eval_path: './data/raw/eval/'          # sample_submission.csv 디렉토리 설정

# Training Settings
epochs: 30                             # (int) The number of training epochs.
train_batch_size: 2048                 # (int) The training batch size.
learner: adam                          # (str) The name of used optimizer.
learning_rate: 0.001                   # (float) Learning rate.
eval_step: 1                           # (int) The number of training epochs before an evaluation on the valid dataset.
stopping_step: 3                       # (int) The threshold for validation-based early stopping.
clip_grad_norm: ~                      # (dict) The args of clip_grad_norm_ which will clip gradient norm of model. 
weight_decay: 0.0                      # (float) The weight decay value (L2 penalty) for optimizers.
loss_decimal_place: 4                  # (int) The decimal place of training loss.
require_pow: False                     # (bool) Whether or not to perform power operation in EmbLoss.
enable_amp: False                      # (bool) Whether or not to use mixed precision training.
enable_scaler: False                   # (bool) Whether or not to use GradScaler in mixed precision training.
transform: ~                           # (str) The transform operation for batch data process.

# Environment Settings
device: cuda
gpu_id: '0'                            # (str) The id of GPU device(s).
worker: 0                              # (int) The number of workers processing the data.
use_gpu: True                          # (bool) Whether or not to use GPU.
seed: 42                               # (int) Random seed.
state: INFO                            # (str) Logging level.
reproducibility: True                  # (bool) Whether or not to make results reproducible.
data_path: './data/processed/'         # (str) The path of input dataset.
output_data_path: './data/output/'
checkpoint_dir: './model/saved/'       # (str) The path to save checkpoint file.
show_progress: True                    # (bool) Whether or not to show the progress bar of every epoch. 
save_dataset: False                    # (bool) Whether or not to save filtered dataset.
dataset_save_path: ~                   # (str) The path of saved dataset.
save_dataloaders: False                # (bool) Whether or not save split dataloaders.
dataloaders_save_path: ~               # (str) The path of saved dataloaders.
log_wandb: False                       # (bool) Whether or not to use Weights & Biases(W&B).
wandb_project: ~                       # (str) The project to conduct experiments in W&B.
shuffle: True                          # (bool) Whether or not to shuffle the training data before each epoch.
dataset: dataset_context